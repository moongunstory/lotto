"""
ë¡œë˜ 6/45 ì¡°í•© ì˜ˆì¸¡ ëª¨ë“ˆ
6ê°œ ë²ˆí˜¸ ì¡°í•©ì„ ì§ì ‘ ì˜ˆì¸¡í•˜ê³  ìŠ¤ì½”ì–´ë§
"""
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from itertools import combinations
import lightgbm as lgb
import optuna

# Optuna ë¡œê¹… ë ˆë²¨ ì„¤ì •
optuna.logging.set_verbosity(optuna.logging.WARNING)


class LottoComboPredictor:
    """ë¡œë˜ ì¡°í•© ì˜ˆì¸¡ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model_type = 'lightgbm'
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.feature_version = None
        
    def _create_model(self, params=None):
        """LightGBM Regressor ëª¨ë¸ ìƒì„±"""
        params = params or {}
        base_params = {
            'random_state': 42,
            'n_jobs': -1,
            'objective': 'regression_l1', # MAE
        }
        final_params = {**base_params, **params}
        return lgb.LGBMRegressor(**final_params)

    def tune_hyperparameters(self, X_train, y_train, X_val, y_val, n_trials=50):
        """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        print(f"\nâš™ï¸ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ({self.model_type}, {n_trials}íšŒ ì‹œë„)")

        def objective(trial):
            param = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'num_leaves': trial.suggest_int('num_leaves', 20, 300),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),
            }
            model = self._create_model(param)
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(10, verbose=False)])
            y_pred = model.predict(X_val)
            return mean_squared_error(y_val, y_pred)

        study = optuna.create_study(direction='minimize')
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        print(f"âœ… íŠœë‹ ì™„ë£Œ! ìµœì  MSE: {study.best_value:.4f}")
        print("   - ìµœì  íŒŒë¼ë¯¸í„°:", study.best_params)
        return study.best_params

    def train(self, feature_engineer, start_draw=100, end_draw=None, 
              negative_samples=5, validation_split=0.2,
              enable_tuning=False, n_trials=50):
        """ì¡°í•© ìŠ¤ì½”ì–´ë§ ëª¨ë¸ í•™ìŠµ"""
        print("\n" + "="*60)
        print(f"ğŸ¯ ì¡°í•© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({self.model_type})")
        print("="*60)
        
        X, y, _ = feature_engineer.build_combo_training_data(
            start_draw=start_draw, end_draw=end_draw, negative_samples=negative_samples
        )
        self.feature_names = X.columns.tolist()
        
        split_idx = int(len(X) * (1 - validation_split))
        X_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"   - í•™ìŠµ: {len(X_train)}ê°œ (ë‹¹ì²¨: {y_train.sum()}ê°œ)")
        print(f"   - ê²€ì¦: {len(X_val)}ê°œ (ë‹¹ì²¨: {y_val.sum()}ê°œ)")
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        best_params = {}
        if enable_tuning:
            best_params = self.tune_hyperparameters(X_train_scaled, y_train, X_val_scaled, y_val, n_trials)

        print(f"\nğŸ”§ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self.model = self._create_model(best_params)
        self.model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], callbacks=[lgb.early_stopping(10, verbose=False)])
        
        y_pred_val = self.model.predict(X_val_scaled)
        val_mse = mean_squared_error(y_val, y_pred_val)
        val_r2 = r2_score(y_val, y_pred_val)
        
        print(f"\nğŸ“ˆ í•™ìŠµ ê²°ê³¼ (ê²€ì¦ ë°ì´í„° ê¸°ì¤€):")
        print(f"   - ê²€ì¦ MSE: {val_mse:.4f} (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        print(f"   - ê²€ì¦ RÂ²: {val_r2:.4f} (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
        
        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        self.feature_version = feature_engineer.get_feature_version()
        return {'val_mse': val_mse, 'val_r2': val_r2}

    def score_combination(self, feature_engineer, numbers, reference_draw=None):
        if self.model is None: raise RuntimeError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # ... (rest of the method is the same)
        if reference_draw is None: reference_draw = feature_engineer.get_latest_draw_number() + 1
        if self.feature_version and self.feature_version != feature_engineer.get_feature_version():
            raise ValueError("í•™ìŠµëœ ì¡°í•© ëª¨ë¸ì˜ í”¼ì²˜ ë²„ì „ì´ í˜„ì¬ ë°ì´í„° ìŠ¤í‚¤ë§ˆì™€ ë‹¤ë¦…ë‹ˆë‹¤.")
        features = feature_engineer.extract_combo_features(numbers, reference_draw)
        X = pd.DataFrame([features])[self.feature_names]
        X_scaled = self.scaler.transform(X)
        score = self.model.predict(X_scaled)[0]
        return float(np.clip(score, 0.0, 1.0))

    def predict_top_combos(self, feature_engineer, number_probabilities, n=10, candidate_pool='smart', pool_size=25, reference_draw=None):
        if reference_draw is None: reference_draw = feature_engineer.get_latest_draw_number() + 1
        # ... (rest of the method is the same)
        candidate_combos = self._generate_candidate_combos(feature_engineer, number_probabilities, candidate_pool, pool_size, n)
        scored_combos = [(combo, self.score_combination(feature_engineer, combo, reference_draw)) for combo in candidate_combos]
        scored_combos.sort(key=lambda x: x[1], reverse=True)
        return scored_combos[:n]

    def _generate_candidate_combos(self, feature_engineer, number_probabilities, mode, pool_size, num_combos):
        if mode == 'smart':
            if not number_probabilities:
                # Fallback to the old method if probabilities are not provided
                print("âš ï¸ ë²ˆí˜¸ í™•ë¥  ì •ë³´ê°€ ì—†ì–´ ê¸°ì¡´ 'í•« ë„˜ë²„' ë°©ì‹ìœ¼ë¡œ í›„ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                recent_freq = feature_engineer.df.tail(50)[['n1','n2','n3','n4','n5','n6']].values.flatten()
                unique, counts = np.unique(recent_freq, return_counts=True)
                top_numbers = [num for num, count in sorted(zip(unique, counts), key=lambda x: x[1], reverse=True)[:pool_size]]
                if len(top_numbers) < 6: return []
                all_combos = list(combinations(top_numbers, 6))
                sample_size = min(len(all_combos), 5000)
                indices = np.random.choice(len(all_combos), size=sample_size, replace=False) if len(all_combos) > 0 else []
                return [list(all_combos[i]) for i in indices]

            # New ideal logic using probabilities
            numbers = list(number_probabilities.keys())
            p_values = np.array(list(number_probabilities.values()))
            
            # Ensure probabilities sum to 1
            p_sum = p_values.sum()
            if p_sum == 0: # Avoid division by zero if all probs are 0
                p_values = np.full(len(numbers), 1/len(numbers))
            else:
                p_values /= p_sum

            # Generate a large number of candidates using weighted sampling
            num_candidates_to_generate = num_combos * 20 
            
            # Use a set to store unique combinations to avoid duplicates during generation
            candidate_set = set()
            # Add a timeout to prevent infinite loops if it's hard to generate unique combos
            max_gen_attempts = num_candidates_to_generate * 5
            attempts = 0
            while len(candidate_set) < num_candidates_to_generate and attempts < max_gen_attempts:
                combo = tuple(sorted(np.random.choice(numbers, size=6, replace=False, p=p_values).tolist()))
                candidate_set.add(combo)
                attempts += 1
            
            return [list(c) for c in candidate_set]

        else: # random
            return [sorted(np.random.choice(range(1, 46), size=6, replace=False).tolist()) for _ in range(num_combos * 20)]

    def save_model(self, path='models/combo_predictor.pkl'):
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        save_data = {
            'model': self.model, 'scaler': self.scaler, 'feature_names': self.feature_names,
            'model_type': self.model_type, 'feature_version': self.feature_version
        }
        with open(path, 'wb') as f: pickle.dump(save_data, f)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")

    def load_model(self, path='models/combo_predictor.pkl', expected_feature_version=None):
        if not Path(path).exists(): raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
        with open(path, 'rb') as f: save_data = pickle.load(f)
        self.model = save_data['model']
        self.scaler = save_data['scaler']
        self.feature_names = save_data['feature_names']
        self.model_type = save_data.get('model_type', 'lightgbm')
        self.feature_version = save_data.get('feature_version')
        if expected_feature_version and self.feature_version != expected_feature_version:
            raise ValueError(f"ì €ì¥ëœ ì¡°í•© ëª¨ë¸ í”¼ì²˜ ë²„ì „({self.feature_version})ê³¼ í˜„ì¬ ë²„ì „({expected_feature_version})ì´ ë‹¤ë¦…ë‹ˆë‹¤.")
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path} (ëª¨ë¸ íƒ€ì…: {self.model_type})")

# (Other methods omitted for brevity)
