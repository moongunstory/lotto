"""
ë¡œë˜ 6/45 ë²ˆí˜¸ë³„ í™•ë¥  ì˜ˆì¸¡ ëª¨ë“ˆ
ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° ë²ˆí˜¸(1~45)ì˜ ë‹¤ìŒ íšŒì°¨ ì¶œí˜„ í™•ë¥  ì˜ˆì¸¡
"""
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import xgboost as xgb


class LottoNumberPredictor:
    """ë¡œë˜ ë²ˆí˜¸ë³„ ì¶œí˜„ í™•ë¥  ì˜ˆì¸¡ í´ë˜ìŠ¤"""
    
    def __init__(self, model_type='xgboost'):
        """
        Args:
            model_type: 'random_forest' or 'xgboost'
        """
        self.model_type = model_type
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.feature_importance = {}
        self.feature_version = None
        
    def _create_model(self):
        """ëª¨ë¸ ìƒì„±"""
        if self.model_type == 'random_forest':
            return RandomForestClassifier(
                n_estimators=200,
                max_depth=10,
                min_samples_split=10,
                min_samples_leaf=5,
                random_state=42,
                n_jobs=-1,
                class_weight='balanced'
            )
        elif self.model_type == 'xgboost':
            return xgb.XGBClassifier(
                n_estimators=200,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                eval_metric='logloss',
                scale_pos_weight=5  # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë³´ì •
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {self.model_type}")
    
    def train(self, feature_engineer, start_draw=100, end_draw=None, validation_split=0.2):
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            feature_engineer: LottoFeatureEngineer ì¸ìŠ¤í„´ìŠ¤
            start_draw: í•™ìŠµ ì‹œì‘ íšŒì°¨
            end_draw: í•™ìŠµ ì¢…ë£Œ íšŒì°¨
            validation_split: ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
        """
        print("\n" + "="*60)
        print(f"ğŸ¤– ë²ˆí˜¸ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({self.model_type})")
        print("="*60)
        
        # 1. í•™ìŠµ ë°ì´í„° ìƒì„±
        X, y, draws = feature_engineer.build_number_training_data(
            start_draw=start_draw,
            end_draw=end_draw
        )
        
        self.feature_names = X.columns.tolist()
        
        # 2. Train/Validation ë¶„í• 
        split_idx = int(len(X) * (1 - validation_split))
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"   - í•™ìŠµ: {len(X_train)}ê°œ (ì¶œí˜„: {y_train.sum()}ê°œ)")
        print(f"   - ê²€ì¦: {len(X_val)}ê°œ (ì¶œí˜„: {y_val.sum()}ê°œ)")
        
        # 3. ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        # 4. ëª¨ë¸ í•™ìŠµ
        print(f"\nğŸ”§ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self.model = self._create_model()
        print("   - XGBoost ëª¨ë¸ í•™ìŠµ. 10 ë¼ìš´ë“œë§ˆë‹¤ ì§„í–‰ ë¡œê·¸ê°€ í‘œì‹œë©ë‹ˆë‹¤.")
        self.model.fit(
            X_train_scaled,
            y_train,
            eval_set=[(X_val_scaled, y_val)],
            verbose=10
        )
        
        # 5. í‰ê°€
        y_pred_train = self.model.predict(X_train_scaled)
        y_pred_val = self.model.predict(X_val_scaled)
        y_prob_val = self.model.predict_proba(X_val_scaled)[:, 1]
        
        train_acc = accuracy_score(y_train, y_pred_train)
        val_acc = accuracy_score(y_val, y_pred_val)
        val_precision = precision_score(y_val, y_pred_val, zero_division=0)
        val_recall = recall_score(y_val, y_pred_val, zero_division=0)
        val_f1 = f1_score(y_val, y_pred_val, zero_division=0)
        
        try:
            val_auc = roc_auc_score(y_val, y_prob_val)
        except:
            val_auc = 0.0
        
        print(f"\nğŸ“ˆ í•™ìŠµ ê²°ê³¼:")
        print(f"   - í•™ìŠµ ì •í™•ë„: {train_acc:.4f}")
        print(f"   - ê²€ì¦ ì •í™•ë„: {val_acc:.4f}")
        print(f"   - ê²€ì¦ ì •ë°€ë„: {val_precision:.4f}")
        print(f"   - ê²€ì¦ ì¬í˜„ìœ¨: {val_recall:.4f}")
        print(f"   - ê²€ì¦ F1: {val_f1:.4f}")
        print(f"   - ê²€ì¦ AUC: {val_auc:.4f}")
        
        # 6. í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°
        if hasattr(self.model, 'feature_importances_'):
            importances = self.model.feature_importances_
            self.feature_importance = dict(zip(self.feature_names, importances))
            
            print(f"\nğŸ” ìƒìœ„ 10ê°œ ì¤‘ìš” í”¼ì²˜:")
            sorted_features = sorted(self.feature_importance.items(), 
                                    key=lambda x: x[1], reverse=True)[:10]
            for feat, importance in sorted_features:
                print(f"   {feat}: {importance:.4f}")
        
        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")

        self.feature_version = feature_engineer.get_feature_version()

        return {
            'train_acc': train_acc,
            'val_acc': val_acc,
            'val_precision': val_precision,
            'val_recall': val_recall,
            'val_f1': val_f1,
            'val_auc': val_auc
        }
    
    def predict_probabilities(self, feature_engineer, draw_no=None):
        """
        ë‹¤ìŒ íšŒì°¨ ê° ë²ˆí˜¸ ì¶œí˜„ í™•ë¥  ì˜ˆì¸¡
        
        Args:
            feature_engineer: LottoFeatureEngineer ì¸ìŠ¤í„´ìŠ¤
            draw_no: ê¸°ì¤€ íšŒì°¨ (Noneì´ë©´ ìµœì‹  íšŒì°¨)
            
        Returns:
            dict: {ë²ˆí˜¸: í™•ë¥ }
        """
        if self.model is None:
            raise RuntimeError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

        current_feature_version = feature_engineer.get_feature_version()
        if self.feature_version and self.feature_version != current_feature_version:
            raise ValueError(
                "í•™ìŠµëœ ëª¨ë¸ì˜ í”¼ì²˜ ë²„ì „ì´ í˜„ì¬ ë°ì´í„° ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•´ì£¼ì„¸ìš”."
            )

        if draw_no is None:
            draw_no = feature_engineer.get_latest_draw_number() + 1

        # í”¼ì²˜ ì¶”ì¶œ
        features_df = feature_engineer.extract_number_features(draw_no)
        
        # ë²ˆí˜¸ ì €ì¥
        numbers = features_df['number'].values
        
        # í”¼ì²˜ë§Œ ì¶”ì¶œ
        X = features_df.drop('number', axis=1)
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”© (í•™ìŠµì‹œì™€ ë™ì¼í•˜ê²Œ)
        if 'range_group' in X.columns:
            X = pd.get_dummies(X, columns=['range_group'], prefix='range')

        current_columns = set(X.columns)
        missing_features = [col for col in self.feature_names if col not in current_columns]

        # range_* í”¼ì²˜ëŠ” ë²”ì£¼ê°€ ë“±ì¥í•˜ì§€ ì•Šì•„ë„ 0ìœ¼ë¡œ ì¶”ê°€í•´ë„ ì•ˆì „í•˜ë‹¤.
        safe_fill_features = [col for col in missing_features if col.startswith('range_')]
        for col in safe_fill_features:
            X[col] = 0

        remaining_missing = [col for col in missing_features if col not in safe_fill_features]
        if remaining_missing:
            preview = ', '.join(remaining_missing[:5])
            if len(remaining_missing) > 5:
                preview += ', ...'
            raise ValueError(
                f"ëª¨ë¸ì´ ì‚¬ìš©í•˜ëŠ” í”¼ì²˜({preview})ë¥¼ í˜„ì¬ ë°ì´í„°ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•´ì£¼ì„¸ìš”."
            )

        X = X[self.feature_names]

        # ìŠ¤ì¼€ì¼ë§
        X_scaled = self.scaler.transform(X)
        
        # í™•ë¥  ì˜ˆì¸¡
        probabilities = self.model.predict_proba(X_scaled)[:, 1]
        
        # ë²ˆí˜¸: í™•ë¥  ë”•ì…”ë„ˆë¦¬ ìƒì„±
        result = {}
        for number, prob in zip(numbers, probabilities):
            result[int(number)] = float(prob)
        
        return result
    
    def get_top_numbers(self, feature_engineer, k=20, draw_no=None):
        """
        í™•ë¥  ë†’ì€ ìƒìœ„ Kê°œ ë²ˆí˜¸ ë°˜í™˜
        
        Args:
            feature_engineer: LottoFeatureEngineer ì¸ìŠ¤í„´ìŠ¤
            k: ë°˜í™˜í•  ë²ˆí˜¸ ê°œìˆ˜
            draw_no: ê¸°ì¤€ íšŒì°¨
            
        Returns:
            list of tuples: [(ë²ˆí˜¸, í™•ë¥ ), ...]
        """
        probabilities = self.predict_probabilities(feature_engineer, draw_no)
        
        sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)
        
        return sorted_probs[:k]
    
    def backtest(self, feature_engineer, test_draws=20):
        """
        ìµœê·¼ NíšŒì°¨ë¡œ ë°±í…ŒìŠ¤íŠ¸
        
        Args:
            feature_engineer: LottoFeatureEngineer ì¸ìŠ¤í„´ìŠ¤
            test_draws: í…ŒìŠ¤íŠ¸í•  íšŒì°¨ ìˆ˜
            
        Returns:
            dict: í‰ê°€ ì§€í‘œ
        """
        print("\n" + "="*60)
        print(f"ğŸ”¬ ë°±í…ŒìŠ¤íŠ¸ ì‹œì‘ (ìµœê·¼ {test_draws}íšŒì°¨)")
        print("="*60)
        
        latest_draw = feature_engineer.get_latest_draw_number()
        start_draw = latest_draw - test_draws + 1
        
        all_predictions = []
        all_actuals = []
        hit_counts = []  # ê° íšŒì°¨ë³„ ë§ì¶˜ ê°œìˆ˜
        top_k_hits = {6: [], 10: [], 15: [], 20: []}  # ìƒìœ„ Kê°œ ì„ íƒì‹œ ì ì¤‘ ê°œìˆ˜
        
        df = feature_engineer.df
        
        for draw_no in range(start_draw, latest_draw + 1):
            # ì˜ˆì¸¡
            probabilities = self.predict_probabilities(feature_engineer, draw_no)
            
            # ì‹¤ì œ ë‹¹ì²¨ë²ˆí˜¸
            if draw_no not in df.index:
                continue
            actual_row = df.loc[draw_no]
            actual_numbers = [
                int(actual_row['n1']), int(actual_row['n2']), int(actual_row['n3']),
                int(actual_row['n4']), int(actual_row['n5']), int(actual_row['n6'])
            ]
            
            # ê° ë²ˆí˜¸ë³„ ì˜ˆì¸¡ vs ì‹¤ì œ
            for number in range(1, 46):
                pred_prob = probabilities[number]
                actual = 1 if number in actual_numbers else 0
                
                all_predictions.append(pred_prob)
                all_actuals.append(actual)
            
            # ìƒìœ„ Kê°œ ì„ íƒì‹œ ì ì¤‘ë¥ 
            sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)
            
            for k in top_k_hits.keys():
                top_k_numbers = [num for num, _ in sorted_probs[:k]]
                hits = len(set(top_k_numbers) & set(actual_numbers))
                top_k_hits[k].append(hits)
        
        # ì „ì²´ í‰ê°€ ì§€í‘œ
        all_predictions = np.array(all_predictions)
        all_actuals = np.array(all_actuals)
        
        # ì„ê³„ê°’ 0.5ë¡œ ì´ì§„ ë¶„ë¥˜
        binary_predictions = (all_predictions > 0.5).astype(int)
        
        accuracy = accuracy_score(all_actuals, binary_predictions)
        precision = precision_score(all_actuals, binary_predictions, zero_division=0)
        recall = recall_score(all_actuals, binary_predictions, zero_division=0)
        f1 = f1_score(all_actuals, binary_predictions, zero_division=0)
        
        try:
            auc = roc_auc_score(all_actuals, all_predictions)
        except:
            auc = 0.0
        
        print(f"\nğŸ“Š ì „ì²´ ì˜ˆì¸¡ ì„±ëŠ¥:")
        print(f"   - ì •í™•ë„: {accuracy:.4f}")
        print(f"   - ì •ë°€ë„: {precision:.4f}")
        print(f"   - ì¬í˜„ìœ¨: {recall:.4f}")
        print(f"   - F1 Score: {f1:.4f}")
        print(f"   - AUC: {auc:.4f}")
        
        print(f"\nğŸ¯ ìƒìœ„ Kê°œ ì„ íƒì‹œ í‰ê·  ì ì¤‘ ê°œìˆ˜:")
        for k, hits in top_k_hits.items():
            avg_hits = np.mean(hits)
            max_hits = np.max(hits)
            print(f"   - ìƒìœ„ {k:2d}ê°œ: í‰ê·  {avg_hits:.2f}ê°œ (ìµœëŒ€ {max_hits}ê°œ)")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc,
            'top_k_performance': {k: np.mean(v) for k, v in top_k_hits.items()}
        }
    
    def save_model(self, path='models/number_predictor.pkl'):
        """ëª¨ë¸ ì €ì¥"""
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        
        save_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'feature_importance': self.feature_importance,
            'model_type': self.model_type,
            'feature_version': self.feature_version
        }

        with open(path, 'wb') as f:
            pickle.dump(save_data, f)

        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")

    def load_model(self, path='models/number_predictor.pkl', expected_feature_version=None):
        """ëª¨ë¸ ë¡œë“œ"""
        if not Path(path).exists():
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")

        with open(path, 'rb') as f:
            save_data = pickle.load(f)

        self.model = save_data['model']
        self.scaler = save_data['scaler']
        self.feature_names = save_data['feature_names']
        self.feature_importance = save_data.get('feature_importance', {})
        self.model_type = save_data.get('model_type', 'unknown')
        self.feature_version = save_data.get('feature_version')

        if expected_feature_version and self.feature_version != expected_feature_version:
            raise ValueError(
                f"ì €ì¥ëœ ëª¨ë¸ í”¼ì²˜ ë²„ì „({self.feature_version})ê³¼ í˜„ì¬ ë²„ì „({expected_feature_version})ì´ ë‹¤ë¦…ë‹ˆë‹¤."
            )

        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
    
    def get_feature_importance(self, top_k=20):
        """
        í”¼ì²˜ ì¤‘ìš”ë„ ë°˜í™˜
        
        Args:
            top_k: ìƒìœ„ Kê°œ ë°˜í™˜
            
        Returns:
            list of tuples: [(í”¼ì²˜ëª…, ì¤‘ìš”ë„), ...]
        """
        if not self.feature_importance:
            return []
        
        sorted_features = sorted(self.feature_importance.items(), 
                                key=lambda x: x[1], reverse=True)
        
        return sorted_features[:top_k]


if __name__ == "__main__":
    from analysis.lotto_feature_engineer import LottoFeatureEngineer

    print("\n" + "="*60)
    print("ğŸ¤– Number Predictor í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # Feature Engineer ìƒì„±
    engineer = LottoFeatureEngineer('lotto/data/lotto_history.csv')
    latest_draw = engineer.get_latest_draw_number()
    
    # Predictor ìƒì„± ë° í•™ìŠµ
    predictor = LottoNumberPredictor(model_type='xgboost')
    
    # í•™ìŠµ (ìµœê·¼ 200íšŒì°¨ ì‚¬ìš©, ë§ˆì§€ë§‰ 20íšŒì°¨ëŠ” í…ŒìŠ¤íŠ¸ìš© ì œì™¸)
    train_end = latest_draw - 20
    train_start = max(100, train_end - 200)
    
    results = predictor.train(
        engineer, 
        start_draw=train_start, 
        end_draw=train_end,
        validation_split=0.2
    )
    
    # ë‹¤ìŒ íšŒì°¨ ì˜ˆì¸¡
    print("\n" + "="*60)
    print(f"ğŸ¯ ë‹¤ìŒ íšŒì°¨ ({latest_draw + 1}íšŒ) ì˜ˆì¸¡")
    print("="*60)
    
    probabilities = predictor.predict_probabilities(engineer)
    top_numbers = predictor.get_top_numbers(engineer, k=20)
    
    print(f"\nğŸ† ìƒìœ„ 20ê°œ ë²ˆí˜¸:")
    for i, (number, prob) in enumerate(top_numbers, 1):
        bar = "â–ˆ" * int(prob * 50)
        print(f"{i:2d}. {number:2d}ë²ˆ  {bar}  {prob*100:.2f}%")
    
    # ë°±í…ŒìŠ¤íŠ¸
    backtest_results = predictor.backtest(engineer, test_draws=20)
    
    # ëª¨ë¸ ì €ì¥
    predictor.save_model('models/number_predictor.pkl')
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
