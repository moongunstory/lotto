"""
ë¡œë˜ 6/45 ë²ˆí˜¸ë³„ í™•ë¥  ì˜ˆì¸¡ ëª¨ë“ˆ
ML ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê° ë²ˆí˜¸(1~45)ì˜ ë‹¤ìŒ íšŒì°¨ ì¶œí˜„ í™•ë¥  ì˜ˆì¸¡
"""
import sys, os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import lightgbm as lgb
import optuna

# Optuna ë¡œê¹… ë ˆë²¨ ì„¤ì •
optuna.logging.set_verbosity(optuna.logging.WARNING)


class LottoNumberPredictor:
    """ë¡œë˜ ë²ˆí˜¸ë³„ ì¶œí˜„ í™•ë¥  ì˜ˆì¸¡ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.model_type = 'lightgbm'
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []
        self.feature_importance = {}
        self.feature_version = None
        
    def _create_model(self, params=None):
        """LightGBM ëª¨ë¸ ìƒì„±"""
        params = params or {}
        base_params = {
            'random_state': 42,
            'n_jobs': -1,
            'objective': 'binary',
        }
        final_params = {**base_params, **params}
        return lgb.LGBMClassifier(**final_params)

    def tune_hyperparameters(self, X_train, y_train, X_val, y_val, n_trials=50):
        """Optunaë¥¼ ì‚¬ìš©í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
        print(f"\nâš™ï¸ Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘ ({self.model_type}, {n_trials}íšŒ ì‹œë„)")

        def objective(trial):
            param = {
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                'num_leaves': trial.suggest_int('num_leaves', 20, 300),
                'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),
                'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),
                'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10),
            }
            model = self._create_model(param)
            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(10, verbose=False)])
            y_prob = model.predict_proba(X_val)[:, 1]
            return roc_auc_score(y_val, y_prob)

        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials, show_progress_bar=True)

        print(f"âœ… íŠœë‹ ì™„ë£Œ! ìµœì  AUC: {study.best_value:.4f}")
        print("   - ìµœì  íŒŒë¼ë¯¸í„°:", study.best_params)
        return study.best_params

    def train(self, feature_engineer, start_draw=100, end_draw=None, validation_split=0.2, 
              enable_tuning=False, n_trials=50):
        """ëª¨ë¸ í•™ìŠµ"""
        print("\n" + "="*60)
        print(f"ğŸ¤– ë²ˆí˜¸ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ({self.model_type})")
        print("="*60)
        
        X, y, _ = feature_engineer.build_number_training_data(start_draw=start_draw, end_draw=end_draw)
        self.feature_names = X.columns.tolist()
        
        split_idx = int(len(X) * (1 - validation_split))
        X_train, X_val = X[:split_idx], X[split_idx:]
        y_train, y_val = y[:split_idx], y[split_idx:]
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"   - í•™ìŠµ: {len(X_train)}ê°œ (ì¶œí˜„: {y_train.sum()}ê°œ)")
        print(f"   - ê²€ì¦: {len(X_val)}ê°œ (ì¶œí˜„: {y_val.sum()}ê°œ)")
        
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        
        best_params = {}
        if enable_tuning:
            best_params = self.tune_hyperparameters(X_train_scaled, y_train, X_val_scaled, y_val, n_trials)

        print(f"\nğŸ”§ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        self.model = self._create_model(best_params)
        
        self.model.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], callbacks=[lgb.early_stopping(10, verbose=False)])
        
        y_pred_val = self.model.predict(X_val_scaled)
        y_prob_val = self.model.predict_proba(X_val_scaled)[:, 1]
        val_auc = roc_auc_score(y_val, y_prob_val) if y_val.sum() > 0 else 0.0
        
        print(f"\nğŸ“ˆ í•™ìŠµ ê²°ê³¼ (ê²€ì¦ AUC): {val_auc:.4f}")
        
        if hasattr(self.model, 'feature_importances_'):
            self.feature_importance = dict(zip(self.feature_names, self.model.feature_importances_))
        
        print("\nâœ… í•™ìŠµ ì™„ë£Œ!")
        self.feature_version = feature_engineer.get_feature_version()
        return {'val_auc': val_auc}

    def predict_probabilities(self, feature_engineer, draw_no=None):
        if self.model is None: raise RuntimeError("ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        # ... (rest of the method is the same)
        current_feature_version = feature_engineer.get_feature_version()
        if self.feature_version and self.feature_version != current_feature_version:
            raise ValueError("í•™ìŠµëœ ëª¨ë¸ì˜ í”¼ì²˜ ë²„ì „ì´ í˜„ì¬ ë°ì´í„° ìŠ¤í‚¤ë§ˆì™€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµí•´ì£¼ì„¸ìš”.")
        if draw_no is None: draw_no = feature_engineer.get_latest_draw_number() + 1
        features_df = feature_engineer.extract_number_features(draw_no)
        numbers = features_df['number'].values
        X = features_df.drop('number', axis=1)
        if 'range_group' in X.columns: X = pd.get_dummies(X, columns=['range_group'], prefix='range')
        missing_features = [col for col in self.feature_names if col not in X.columns]
        for col in missing_features: X[col] = 0
        X = X[self.feature_names]
        X_scaled = self.scaler.transform(X)
        probabilities = self.model.predict_proba(X_scaled)[:, 1]
        return dict(zip(map(int, numbers), map(float, probabilities)))

    def save_model(self, path='models/number_predictor.pkl'):
        Path(path).parent.mkdir(parents=True, exist_ok=True)
        save_data = {
            'model': self.model, 'scaler': self.scaler, 'feature_names': self.feature_names,
            'feature_importance': self.feature_importance, 'model_type': self.model_type,
            'feature_version': self.feature_version
        }
        with open(path, 'wb') as f: pickle.dump(save_data, f)
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")

    def load_model(self, path='models/number_predictor.pkl', expected_feature_version=None):
        if not Path(path).exists(): raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
        with open(path, 'rb') as f: save_data = pickle.load(f)
        self.model = save_data['model']
        self.scaler = save_data['scaler']
        self.feature_names = save_data['feature_names']
        self.feature_importance = save_data.get('feature_importance', {})
        self.model_type = save_data.get('model_type', 'lightgbm')
        self.feature_version = save_data.get('feature_version')
        if expected_feature_version and self.feature_version != expected_feature_version:
            raise ValueError(f"ì €ì¥ëœ ëª¨ë¸ í”¼ì²˜ ë²„ì „({self.feature_version})ê³¼ í˜„ì¬ ë²„ì „({expected_feature_version})ì´ ë‹¤ë¦…ë‹ˆë‹¤.")
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path} (ëª¨ë¸ íƒ€ì…: {self.model_type})")

# Other methods like get_top_numbers, get_feature_importance can remain mostly the same
# ... (omitted for brevity)

